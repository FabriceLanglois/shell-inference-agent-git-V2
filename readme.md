# Assistant IA Ollama

Une interface web moderne pour interagir avec des modÃ¨les d'IA en local via Ollama.

![Version](https://img.shields.io/badge/version-1.1.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## ğŸ“‹ PrÃ©sentation

Assistant IA Ollama est une application web dÃ©veloppÃ©e en Flask qui permet d'interagir facilement avec des modÃ¨les d'IA de langage (LLM) exÃ©cutÃ©s localement via [Ollama](https://ollama.com/). Cette application offre deux interfaces principales :

1. **Console Interactive** : Une interface de type terminal pour exÃ©cuter des commandes et interagir avec les modÃ¨les d'IA.
2. **Gestionnaire de ModÃ¨les** : Une interface conviviale pour tÃ©lÃ©charger, tester et gÃ©rer les modÃ¨les Ollama.

## âœ¨ FonctionnalitÃ©s

- ğŸ–¥ï¸ **Console Interactive**
  - ExÃ©cution de commandes shell
  - InfÃ©rence directe avec les modÃ¨les d'IA
  - Mode interactif (SSH, etc.)
  - Historique des commandes
  - Exploration de fichiers intÃ©grÃ©e

- ğŸ§  **Gestion des ModÃ¨les**
  - TÃ©lÃ©chargement de nouveaux modÃ¨les
  - Test des modÃ¨les avec diffÃ©rents paramÃ¨tres
  - DÃ©finition du modÃ¨le par dÃ©faut
  - Suppression des modÃ¨les
  - Statistiques d'utilisation

- ğŸ¨ **Interface Utilisateur**
  - Design rÃ©actif et moderne
  - ThÃ¨mes clair et sombre
  - Notifications toast
  - Interface intuitive et cohÃ©rente

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- [Ollama](https://ollama.com/) installÃ© sur votre systÃ¨me (ou accessible via rÃ©seau)

### Installation automatique

1. Clonez ce dÃ©pÃ´t :
   ```bash
   git clone https://github.com/votre-username/assistant-ia-ollama.git
   cd assistant-ia-ollama
   ```

2. ExÃ©cutez le script d'installation :
   ```bash
   chmod +x setup-environment.sh
   ./setup-environment.sh
   ```

3. Si vous n'avez pas encore Ollama, installez-le :
   ```bash
   chmod +x install-ollama.sh
   ./install-ollama.sh
   ```

### Installation manuelle

1. Clonez ce dÃ©pÃ´t :
   ```bash
   git clone https://github.com/votre-username/assistant-ia-ollama.git
   cd assistant-ia-ollama
   ```

2. CrÃ©ez et activez un environnement virtuel :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   ```

3. Installez les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

## ğŸƒâ€â™‚ï¸ Utilisation

1. Activez l'environnement virtuel si ce n'est pas dÃ©jÃ  fait :
   ```bash
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   ```

2. DÃ©marrez le service Ollama (dans un terminal sÃ©parÃ©) :
   ```bash
   ollama serve
   ```

3. Lancez l'application :
   ```bash
   python app.py
   ```

4. Ouvrez votre navigateur et accÃ©dez Ã  :
   ```
   http://localhost:5000
   ```

## ğŸ” Diagnostic et rÃ©solution des problÃ¨mes

Si vous rencontrez des problÃ¨mes, l'application inclut un utilitaire de diagnostic qui peut vous aider Ã  les identifier et les rÃ©soudre :

```bash
python diagnostic.py
```

Cet outil vÃ©rifiera :
- L'installation d'Ollama
- L'Ã©tat du service Ollama
- Les modÃ¨les disponibles
- La configuration de l'application
- Les dÃ©pendances Python
- La structure des rÃ©pertoires
- Et plus encore...

Vous pouvez Ã©galement utiliser des options spÃ©cifiques :

```bash
# VÃ©rifier uniquement l'installation d'Ollama
python diagnostic.py --check ollama

# DÃ©marrer le service Ollama
python diagnostic.py --start-ollama

# TÃ©lÃ©charger un modÃ¨le spÃ©cifique
python diagnostic.py --download-model llama3

# RÃ©initialiser la configuration
python diagnostic.py --reset-config
```

## ğŸ“Š Structure du Projet

```
assistant-ia-ollama/
â”œâ”€â”€ app.py                  # Application Flask principale
â”œâ”€â”€ run-inference.py        # Script d'infÃ©rence avec Ollama
â”œâ”€â”€ manage-models.py        # Gestionnaire de modÃ¨les Ollama
â”œâ”€â”€ diagnostic.py           # Utilitaire de diagnostic et rÃ©solution des problÃ¨mes
â”œâ”€â”€ setup-environment.sh    # Script d'installation de l'environnement
â”œâ”€â”€ install-ollama.sh       # Script d'installation d'Ollama
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python pour le projet
â”œâ”€â”€ .gitignore              # Configuration Git
â”œâ”€â”€ README.md               # Documentation du projet
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”œâ”€â”€ main.css        # Styles partagÃ©s
â”‚   â”‚   â”œâ”€â”€ console.css     # Styles pour la console
â”‚   â”‚   â””â”€â”€ ollama.css      # Styles pour la page Ollama
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ common.js       # Fonctions JS partagÃ©es
â”‚   â”‚   â”œâ”€â”€ console.js      # Fonctions JS pour la console
â”‚   â”‚   â””â”€â”€ ollama.js       # Fonctions JS pour la page Ollama
â”‚   â””â”€â”€ img/
â”‚       â””â”€â”€ chart-placeholder.png  # Image pour les graphiques
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html          # Interface console principale
â”‚   â””â”€â”€ ollama_manager.html # Interface de gestion Ollama
â””â”€â”€ stats/                  # RÃ©pertoire pour les statistiques d'infÃ©rence
```

## ğŸ› ï¸ API REST

L'application expose plusieurs endpoints API REST pour interagir avec Ollama :

- **GET** `/api/models` : Liste des modÃ¨les disponibles
- **GET** `/api/current-model` : ModÃ¨le actuellement sÃ©lectionnÃ©
- **POST** `/api/download-model` : TÃ©lÃ©charger un nouveau modÃ¨le
- **POST** `/api/delete-model` : Supprimer un modÃ¨le
- **POST** `/api/set-default-model` : DÃ©finir le modÃ¨le par dÃ©faut
- **POST** `/api/test-model` : Tester un modÃ¨le avec un prompt
- **GET** `/api/stats/inference-history` : Historique des infÃ©rences
- **GET** `/api/stats/model-usage` : Statistiques d'utilisation des modÃ¨les
- **GET** `/api/stats/performance` : Statistiques de performance
- **GET** `/api/gpu-info` : Informations sur le GPU
- **GET** `/api/diagnostic` : Informations de diagnostic sur l'application

## ğŸ–¥ï¸ CompatibilitÃ© GPU

L'application est conÃ§ue pour utiliser automatiquement un GPU NVIDIA si disponible. VÃ©rifiez que :

1. Vous avez installÃ© les pilotes NVIDIA appropriÃ©s
2. CUDA est correctement configurÃ©
3. PyTorch est installÃ© avec le support CUDA

## âš ï¸ RÃ©solution des problÃ¨mes courants

- **Ollama n'est pas en cours d'exÃ©cution** : DÃ©marrez le service avec `ollama serve` dans un terminal sÃ©parÃ©
- **Aucun modÃ¨le n'est disponible** : TÃ©lÃ©chargez un modÃ¨le via l'interface de gestion ou avec `ollama pull llama3`
- **L'application ne dÃ©marre pas** : VÃ©rifiez que toutes les dÃ©pendances sont installÃ©es avec `pip install -r requirements.txt`
- **Erreur de connexion** : VÃ©rifiez que le service Ollama est en cours d'exÃ©cution sur `localhost:11434`
- **ProblÃ¨mes de configuration** : Utilisez l'utilitaire de diagnostic avec `python diagnostic.py`

## ğŸ”§ Notes pour les dÃ©veloppeurs

Si vous souhaitez contribuer au dÃ©veloppement :

1. VÃ©rifiez que vous utilisez bien l'environnement virtuel
2. ExÃ©cutez les tests unitaires si disponibles
3. Suivez les bonnes pratiques de codage Python (PEP 8)
4. Documentez correctement les nouvelles fonctionnalitÃ©s
5. Testez sur diffÃ©rentes plateformes (Linux, Windows, macOS) si possible

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Pour contribuer :

1. Forkez le dÃ©pÃ´t
2. CrÃ©ez une branche pour votre fonctionnalitÃ© (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Committez vos changements (`git commit -am 'Ajout d'une nouvelle fonctionnalitÃ©'`)
4. Poussez vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Ouvrez une Pull Request

## ğŸ“œ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ“ Support

Si vous rencontrez des problÃ¨mes ou avez des questions, veuillez ouvrir un ticket dans la section "Issues" du dÃ©pÃ´t GitHub.

Pour des problÃ¨mes spÃ©cifiques Ã  Ollama, consultez la [documentation d'Ollama](https://ollama.com/blog/getting-started).

---

DÃ©veloppÃ© avec â¤ï¸ par [Votre Nom]