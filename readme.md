# Assistant IA Ollama v2.0

<div align="center">

![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)
![Flask](https://img.shields.io/badge/Flask-2.3.3-green.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Docker](https://img.shields.io/badge/Docker-supported-blue.svg)

Une interface web moderne et Ã©lÃ©gante pour interagir avec des modÃ¨les d'IA en local via Ollama.

[Installation](#-installation) â€¢ 
[FonctionnalitÃ©s](#-fonctionnalitÃ©s) â€¢  
[Docker](#-utilisation-avec-docker) â€¢ 
[Documentation](#-documentation) â€¢ 
[Tests](#-tests) â€¢ 
[Contribution](#-contribution)

<p align="center">
  <img src="static/img/screenshot.png" alt="Assistant IA Ollama" width="800">
</p>

</div>

## ğŸ“‹ PrÃ©sentation

Assistant IA Ollama est une application web dÃ©veloppÃ©e en Flask qui permet d'interagir facilement avec des modÃ¨les de langage (LLM) exÃ©cutÃ©s localement via [Ollama](https://ollama.com/). Cette application offre deux interfaces principales :

1. **Console Interactive** : Une interface de type terminal pour exÃ©cuter des commandes et interagir avec les modÃ¨les d'IA.
2. **Gestionnaire de ModÃ¨les** : Une interface conviviale pour tÃ©lÃ©charger, tester et gÃ©rer les modÃ¨les Ollama.

## ğŸš€ Installation

### Option 1: Installation automatique (recommandÃ©e)

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/votre-username/assistant-ia-ollama.git
cd assistant-ia-ollama

# ExÃ©cuter le script d'installation complet
chmod +x install-all.sh
./install-all.sh
```

Ce script configure l'environnement Python, installe Ollama si nÃ©cessaire, et effectue toutes les configurations requises.

### Option 2: Installation manuelle Ã©tape par Ã©tape

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/votre-username/assistant-ia-ollama.git
cd assistant-ia-ollama

# Configurer l'environnement Python
chmod +x setup-environment.sh
./setup-environment.sh

# Activer l'environnement virtuel
source venv/bin/activate

# Installer Ollama si nÃ©cessaire
chmod +x install-ollama.sh
./install-ollama.sh
```

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- [Ollama](https://ollama.com/) installÃ© sur votre systÃ¨me (ou accessible via rÃ©seau)
- Navigateur web moderne

## ğŸƒâ€â™‚ï¸ Utilisation

```bash
# Activer l'environnement virtuel
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Lancer l'application
python app.py
```

Ouvrez votre navigateur et accÃ©dez Ã  :
```
http://localhost:5000
```

## ğŸ³ Utilisation avec Docker

### Option 1: Docker Compose (recommandÃ©e)

```bash
# Lancer l'application et Ollama ensemble
docker-compose up -d

# Pour arrÃªter les services
docker-compose down
```

### Option 2: Dockerfile personnalisÃ©

```bash
# Construire l'image
docker build -t assistant-ia-ollama .

# Lancer le conteneur (en supposant qu'Ollama est exÃ©cutÃ© sur la machine hÃ´te)
docker run -p 5000:5000 assistant-ia-ollama
```

## âœ¨ FonctionnalitÃ©s

### ğŸ–¥ï¸ Console Interactive
- **ExÃ©cution de commandes shell** avec affichage en temps rÃ©el
- **InfÃ©rence directe** avec les modÃ¨les d'IA locaux
- **Mode interactif** pour les sessions persistantes (SSH, etc.)
- **Historique des commandes** avec navigation facile
- **Exploration de fichiers intÃ©grÃ©e** avec navigation visuelle
- **Copie intelligente** du contenu du terminal
- **Raccourcis clavier** pour une utilisation efficace
- **Reconnaissance vocale** pour la saisie de commandes (navigateurs compatibles)

### ğŸ§  Gestion des ModÃ¨les
- **TÃ©lÃ©chargement simplifiÃ©** de nouveaux modÃ¨les depuis la bibliothÃ¨que Ollama
- **Interface de test** avec paramÃ¨tres ajustables (tempÃ©rature, tokens)
- **Gestion des modÃ¨les par dÃ©faut** pour une configuration personnalisÃ©e
- **Suppression sÃ©curisÃ©e** des modÃ¨les inutilisÃ©s
- **Statistiques d'utilisation** dÃ©taillÃ©es par modÃ¨le

### ğŸ¨ Interface Utilisateur
- **Design responsive** adaptÃ© Ã  tous les appareils
- **ThÃ¨mes clair et sombre** avec dÃ©tection automatique des prÃ©fÃ©rences
- **AccessibilitÃ© amÃ©liorÃ©e** conforme aux standards WCAG
- **Notifications toast** pour les retours utilisateur
- **Modals interactifs** pour les actions complexes

## ğŸ› ï¸ API REST

L'application expose plusieurs endpoints API REST pour interagir avec Ollama :

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/api/models` | Liste des modÃ¨les disponibles |
| GET | `/api/current-model` | ModÃ¨le actuellement sÃ©lectionnÃ© |
| POST | `/api/download-model` | TÃ©lÃ©charger un nouveau modÃ¨le |
| POST | `/api/delete-model` | Supprimer un modÃ¨le |
| POST | `/api/set-default-model` | DÃ©finir le modÃ¨le par dÃ©faut |
| POST | `/api/test-model` | Tester un modÃ¨le avec un prompt |
| GET | `/api/stats/inference-history` | Historique des infÃ©rences |
| GET | `/api/stats/model-usage` | Statistiques d'utilisation des modÃ¨les |
| GET | `/api/stats/performance` | Statistiques de performance |
| GET | `/api/gpu-info` | Informations sur le GPU |

## ğŸ–¥ï¸ CompatibilitÃ© GPU

L'application est conÃ§ue pour utiliser automatiquement un GPU NVIDIA si disponible. VÃ©rifiez que :

1. Vous avez installÃ© les pilotes NVIDIA appropriÃ©s
2. CUDA est correctement configurÃ©
3. PyTorch est installÃ© avec le support CUDA

## ğŸ“Š Structure du Projet

```
assistant-ia-ollama/
â”œâ”€â”€ app.py                  # Application Flask principale
â”œâ”€â”€ run-inference.py        # Script d'infÃ©rence avec Ollama
â”œâ”€â”€ manage-models.py        # Gestionnaire de modÃ¨les Ollama
â”œâ”€â”€ setup-environment.sh    # Script d'installation de l'environnement
â”œâ”€â”€ install-ollama.sh       # Script d'installation d'Ollama
â”œâ”€â”€ install-all.sh          # Script d'installation complet
â”œâ”€â”€ Dockerfile              # Configuration Docker
â”œâ”€â”€ docker-compose.yml      # Configuration Docker Compose
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python pour le projet
â”œâ”€â”€ config.json             # Configuration centrale de l'application
â”œâ”€â”€ .env.example            # Template pour les variables d'environnement
â”œâ”€â”€ test_app.py             # Tests unitaires
â”œâ”€â”€ pytest.ini              # Configuration pour pytest
â”œâ”€â”€ LICENSE                 # Licence MIT
â”œâ”€â”€ README.md               # Documentation du projet
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”œâ”€â”€ main.css        # Styles partagÃ©s
â”‚   â”‚   â”œâ”€â”€ console.css     # Styles pour la console
â”‚   â”‚   â””â”€â”€ ollama.css      # Styles pour la page Ollama
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ common.js       # Fonctions JS partagÃ©es
â”‚   â”‚   â”œâ”€â”€ console.js      # Fonctions JS pour la console
â”‚   â”‚   â””â”€â”€ ollama.js       # Fonctions JS pour la page Ollama
â”‚   â””â”€â”€ img/                # Images et ressources visuelles
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html          # Interface console principale
â”‚   â”œâ”€â”€ ollama_manager.html # Interface de gestion Ollama
â”‚   â”œâ”€â”€ 404.html            # Page d'erreur 404
â”‚   â””â”€â”€ 500.html            # Page d'erreur 500
â”œâ”€â”€ logs/                   # Logs applicatifs
â””â”€â”€ stats/                  # Statistiques d'utilisation
```

## ğŸ” Configuration

### Variables d'environnement

Copiez le fichier `.env.example` vers `.env` et ajustez les paramÃ¨tres selon vos besoins :

```bash
cp .env.example .env
```

Options principales :
- `FLASK_SECRET_KEY` : ClÃ© secrÃ¨te pour sÃ©curiser les sessions
- `OLLAMA_HOST` : HÃ´te oÃ¹ Ollama est exÃ©cutÃ© (par dÃ©faut: localhost)
- `OLLAMA_PORT` : Port Ollama (par dÃ©faut: 11434)
- `DEFAULT_MODEL` : ModÃ¨le Ã  utiliser par dÃ©faut

### Fichier de configuration

Le fichier `config.json` contient des paramÃ¨tres avancÃ©s pour l'application. Vous pouvez le modifier pour personnaliser :

- Les paramÃ¨tres du serveur
- Les options d'infÃ©rence par dÃ©faut
- Les paramÃ¨tres de journalisation
- Les prÃ©fÃ©rences d'interface utilisateur

## ğŸ§ª Tests

L'application est fournie avec une suite de tests unitaires pour vÃ©rifier son bon fonctionnement :

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# ExÃ©cuter tous les tests
pytest

# ExÃ©cuter des tests spÃ©cifiques
pytest test_app.py -k test_api
```

## ğŸ” Troubleshooting

### ProblÃ¨mes courants et solutions

1. **Ollama n'est pas dÃ©tectÃ©**
   - VÃ©rifiez qu'Ollama est bien installÃ© avec `ollama --version`
   - Assurez-vous que le service Ollama est en cours d'exÃ©cution avec `ollama serve`
   - VÃ©rifiez les logs dans `logs/app.log` pour plus de dÃ©tails

2. **Erreurs d'infÃ©rence**
   - VÃ©rifiez que vous avez tÃ©lÃ©chargÃ© au moins un modÃ¨le avec `ollama list`
   - Assurez-vous que votre GPU a suffisamment de mÃ©moire
   - Consultez les logs dans `logs/inference.log`

3. **Page blanche ou CSS non chargÃ©**
   - Effacez le cache de votre navigateur
   - VÃ©rifiez la console dÃ©veloppeur pour les erreurs
   - RedÃ©marrez l'application Flask

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Pour contribuer :

1. Forkez le dÃ©pÃ´t
2. CrÃ©ez une branche pour votre fonctionnalitÃ© (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Committez vos changements (`git commit -am 'Ajout d'une nouvelle fonctionnalitÃ©'`)
4. Poussez vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Ouvrez une Pull Request

Veuillez vous assurer que vos contributions :
- Respectent le style de code du projet
- Incluent des tests unitaires pour les nouvelles fonctionnalitÃ©s
- Mettent Ã  jour la documentation si nÃ©cessaire
- Passent tous les tests automatisÃ©s

## ğŸ“ AmÃ©liorations v2.0

Cette version 2.0 apporte de nombreuses amÃ©liorations par rapport Ã  la v1.0 :

- **Architecture refactorisÃ©e** pour une meilleure maintenabilitÃ©
- **Code optimisÃ©** avec gestion d'erreurs amÃ©liorÃ©e
- **Interface utilisateur modernisÃ©e** avec une meilleure ergonomie
- **AccessibilitÃ© WCAG** intÃ©grÃ©e pour tous les utilisateurs
- **Mode sombre/clair** avec transitions fluides
- **SÃ©curitÃ© renforcÃ©e** contre les injections et vulnÃ©rabilitÃ©s
- **Support Docker** pour un dÃ©ploiement simplifiÃ©
- **Documentation complÃ¨te** du code et des APIs
- **Tests unitaires** pour les fonctions critiques

## ğŸ“– Documentation

Une documentation dÃ©taillÃ©e est disponible dans le code source et les commentaires. Pour une aide plus approfondie :

- Consultez les docstrings dans les fichiers Python
- Explorez le dossier `docs/` (si ajoutÃ© ultÃ©rieurement)
- Consultez le wiki du projet sur GitHub (si disponible)

## ğŸ“œ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ“ Support

Si vous rencontrez des problÃ¨mes ou avez des questions, veuillez ouvrir un ticket dans la section "Issues" du dÃ©pÃ´t GitHub.

---

<div align="center">
DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© de l'IA open-source

**InspirÃ© par la puissance des modÃ¨les d'IA locaux et la simplicitÃ© d'Ollama**
</div>