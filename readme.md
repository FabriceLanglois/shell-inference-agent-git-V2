# Assistant IA Ollama v2.0

Une interface web moderne et Ã©lÃ©gante pour interagir avec des modÃ¨les d'IA en local via Ollama.

![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

<p align="center">
  <img src="static/img/screenshot.png" alt="Assistant IA Ollama" width="800">
</p>

## ğŸ“‹ PrÃ©sentation

Assistant IA Ollama est une application web dÃ©veloppÃ©e en Flask qui permet d'interagir facilement avec des modÃ¨les de langage (LLM) exÃ©cutÃ©s localement via [Ollama](https://ollama.com/). Cette application offre deux interfaces principales :

1. **Console Interactive** : Une interface de type terminal pour exÃ©cuter des commandes et interagir avec les modÃ¨les d'IA.
2. **Gestionnaire de ModÃ¨les** : Une interface conviviale pour tÃ©lÃ©charger, tester et gÃ©rer les modÃ¨les Ollama.

## âœ¨ FonctionnalitÃ©s

### ğŸ–¥ï¸ Console Interactive
- **ExÃ©cution de commandes shell** avec affichage en temps rÃ©el
- **InfÃ©rence directe** avec les modÃ¨les d'IA locaux
- **Mode interactif** pour les sessions persistantes (SSH, etc.)
- **Historique des commandes** avec navigation facile
- **Exploration de fichiers intÃ©grÃ©e** avec navigation visuelle
- **Copie intelligente** du contenu du terminal
- **Raccourcis clavier** pour une utilisation efficace

### ğŸ§  Gestion des ModÃ¨les
- **TÃ©lÃ©chargement simplifiÃ©** de nouveaux modÃ¨les depuis la bibliothÃ¨que Ollama
- **Interface de test** avec paramÃ¨tres ajustables (tempÃ©rature, tokens)
- **Gestion des modÃ¨les par dÃ©faut** pour une configuration personnalisÃ©e
- **Suppression sÃ©curisÃ©e** des modÃ¨les inutilisÃ©s
- **Statistiques d'utilisation** dÃ©taillÃ©es par modÃ¨le

### ğŸ¨ Interface Utilisateur
- **Design responsive** adaptÃ© Ã  tous les appareils
- **ThÃ¨mes clair et sombre** avec dÃ©tection automatique des prÃ©fÃ©rences
- **AccessibilitÃ© amÃ©liorÃ©e** conforme aux standards WCAG
- **Notifications toast** pour les retours utilisateur
- **Modals interactifs** pour les actions complexes

## ğŸ–¼ï¸ Captures d'Ã©cran

<p align="center">
  <img src="static/img/console.png" alt="Console Interactive" width="400" style="margin-right: 10px;">
  <img src="static/img/models.png" alt="Gestionnaire de ModÃ¨les" width="400">
</p>

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- [Ollama](https://ollama.com/) installÃ© sur votre systÃ¨me (ou accessible via rÃ©seau)

### Installation automatique

1. Clonez ce dÃ©pÃ´t :
   ```bash
   git clone https://github.com/votre-username/assistant-ia-ollama.git
   cd assistant-ia-ollama
   ```

2. ExÃ©cutez le script d'installation :
   ```bash
   chmod +x setup-environment.sh
   ./setup-environment.sh
   ```

3. Si vous n'avez pas encore Ollama, installez-le :
   ```bash
   chmod +x install-ollama.sh
   ./install-ollama.sh
   ```

### Installation manuelle

1. Clonez ce dÃ©pÃ´t :
   ```bash
   git clone https://github.com/votre-username/assistant-ia-ollama.git
   cd assistant-ia-ollama
   ```

2. CrÃ©ez et activez un environnement virtuel :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   ```

3. Installez les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

4. CrÃ©ez les rÃ©pertoires requis :
   ```bash
   mkdir -p templates static/img static/css static/js stats logs
   ```

5. Initialisez les fichiers de configuration :
   ```bash
   echo '[]' > stats/inference_stats.json
   echo '{"default_model": "llama3"}' > ollama_config.json
   ```

## ğŸƒâ€â™‚ï¸ Utilisation

1. Activez l'environnement virtuel si ce n'est pas dÃ©jÃ  fait :
   ```bash
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   ```

2. Lancez l'application :
   ```bash
   python app.py
   ```

3. Ouvrez votre navigateur et accÃ©dez Ã  :
   ```
   http://localhost:5000
   ```

## ğŸ“Š Structure du Projet

```
assistant-ia-ollama/
â”œâ”€â”€ app.py                  # Application Flask principale
â”œâ”€â”€ run-inference.py        # Script d'infÃ©rence avec Ollama
â”œâ”€â”€ manage-models.py        # Gestionnaire de modÃ¨les Ollama
â”œâ”€â”€ setup-environment.sh    # Script d'installation de l'environnement
â”œâ”€â”€ install-ollama.sh       # Script d'installation d'Ollama
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python pour le projet
â”œâ”€â”€ README.md               # Documentation du projet
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”œâ”€â”€ main.css        # Styles partagÃ©s
â”‚   â”‚   â”œâ”€â”€ console.css     # Styles pour la console
â”‚   â”‚   â””â”€â”€ ollama.css      # Styles pour la page Ollama
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ common.js       # Fonctions JS partagÃ©es
â”‚   â”‚   â”œâ”€â”€ console.js      # Fonctions JS pour la console
â”‚   â”‚   â””â”€â”€ ollama.js       # Fonctions JS pour la page Ollama
â”‚   â””â”€â”€ img/                # Images et ressources visuelles
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html          # Interface console principale
â”‚   â”œâ”€â”€ ollama_manager.html # Interface de gestion Ollama
â”‚   â”œâ”€â”€ 404.html            # Page d'erreur 404
â”‚   â””â”€â”€ 500.html            # Page d'erreur 500
â”œâ”€â”€ logs/                   # Logs applicatifs
â””â”€â”€ stats/                  # Statistiques d'utilisation
```

## ğŸ› ï¸ API REST

L'application expose plusieurs endpoints API REST pour interagir avec Ollama :

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/api/models` | Liste des modÃ¨les disponibles |
| GET | `/api/current-model` | ModÃ¨le actuellement sÃ©lectionnÃ© |
| POST | `/api/download-model` | TÃ©lÃ©charger un nouveau modÃ¨le |
| POST | `/api/delete-model` | Supprimer un modÃ¨le |
| POST | `/api/set-default-model` | DÃ©finir le modÃ¨le par dÃ©faut |
| POST | `/api/test-model` | Tester un modÃ¨le avec un prompt |
| GET | `/api/stats/inference-history` | Historique des infÃ©rences |
| GET | `/api/stats/model-usage` | Statistiques d'utilisation des modÃ¨les |
| GET | `/api/stats/performance` | Statistiques de performance |
| GET | `/api/gpu-info` | Informations sur le GPU |

## ğŸ–¥ï¸ CompatibilitÃ© GPU

L'application est conÃ§ue pour utiliser automatiquement un GPU NVIDIA si disponible. VÃ©rifiez que :

1. Vous avez installÃ© les pilotes NVIDIA appropriÃ©s
2. CUDA est correctement configurÃ©
3. PyTorch est installÃ© avec le support CUDA

## ğŸ” Troubleshooting

### ProblÃ¨mes courants et solutions

1. **Ollama n'est pas dÃ©tectÃ©**
   - VÃ©rifiez qu'Ollama est bien installÃ© avec `ollama --version`
   - Assurez-vous que le service Ollama est en cours d'exÃ©cution avec `ollama serve`
   - VÃ©rifiez les logs dans `logs/app.log` pour plus de dÃ©tails

2. **Erreurs d'infÃ©rence**
   - VÃ©rifiez que vous avez tÃ©lÃ©chargÃ© au moins un modÃ¨le avec `ollama list`
   - Assurez-vous que votre GPU a suffisamment de mÃ©moire
   - Consultez les logs dans `logs/inference.log`

3. **Page blanche ou CSS non chargÃ©**
   - Effacez le cache de votre navigateur
   - VÃ©rifiez la console dÃ©veloppeur pour les erreurs
   - RedÃ©marrez l'application Flask

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Pour contribuer :

1. Forkez le dÃ©pÃ´t
2. CrÃ©ez une branche pour votre fonctionnalitÃ© (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Committez vos changements (`git commit -am 'Ajout d'une nouvelle fonctionnalitÃ©'`)
4. Poussez vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Ouvrez une Pull Request

## ğŸ“ AmÃ©liorations v2.0

Cette version 2.0 apporte de nombreuses amÃ©liorations par rapport Ã  la v1.0 :

- **Architecture refactorisÃ©e** pour une meilleure maintenabilitÃ©
- **Code optimisÃ©** avec gestion d'erreurs amÃ©liorÃ©e
- **Interface utilisateur modernisÃ©e** avec une meilleure ergonomie
- **AccessibilitÃ© WCAG** intÃ©grÃ©e pour tous les utilisateurs
- **Mode sombre/clair** avec transitions fluides
- **SÃ©curitÃ© renforcÃ©e** contre les injections et vulnÃ©rabilitÃ©s
- **Documentation complÃ¨te** du code et des APIs
- **Tests unitaires** pour les fonctions critiques

## ğŸ“œ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ“ Support

Si vous rencontrez des problÃ¨mes ou avez des questions, veuillez ouvrir un ticket dans la section "Issues" du dÃ©pÃ´t GitHub.

---

DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© de l'IA open-source
